name: Performance Testing

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main, develop]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: call_coaching
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install pytest-benchmark

      - name: Set up test database
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/call_coaching
        run: |
          psql $DATABASE_URL -c "CREATE SCHEMA IF NOT EXISTS public;"

      - name: Start API server
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/call_coaching
          REDIS_URL: redis://localhost:6379
        run: |
          python -m uvicorn api.rest_server:app --host 127.0.0.1 --port 8000 &
          sleep 10
          curl http://127.0.0.1:8000/health || echo "API not ready yet"

      - name: Run API benchmarks
        run: |
          pytest benchmarks/api_benchmarks.py \
            --benchmark-json=benchmarks/.benchmarks/api_results.json \
            --benchmark-compare=benchmarks/.benchmarks/api_baseline.json \
            --benchmark-compare-fail=mean:10% \
            -v

      - name: Run database benchmarks
        run: |
          pytest benchmarks/db_benchmarks.py \
            --benchmark-json=benchmarks/.benchmarks/db_results.json \
            --benchmark-compare=benchmarks/.benchmarks/db_baseline.json \
            --benchmark-compare-fail=mean:10% \
            -v

      - name: Run cache benchmarks
        run: |
          pytest benchmarks/cache_benchmarks.py \
            --benchmark-json=benchmarks/.benchmarks/cache_results.json \
            --benchmark-compare=benchmarks/.benchmarks/cache_baseline.json \
            --benchmark-compare-fail=mean:10% \
            -v

      - name: Run performance scenarios
        run: |
          pytest tests/performance/scenarios/ \
            -v \
            -m performance \
            --tb=short

      - name: Generate performance report
        if: always()
        run: |
          python benchmarks/report.py

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmarks/.benchmarks/
          retention-days: 30

      - name: Upload performance report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-report
          path: benchmarks/reports/
          retention-days: 30

      - name: Comment PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            let comment = '## Performance Test Results\n\n';

            // Check if benchmarks passed
            const benchmarkPath = 'benchmarks/reports/performance_report.html';
            if (fs.existsSync(benchmarkPath)) {
              comment += '‚úÖ Performance tests completed\n';
              comment += 'üìä [View detailed report](artifacts)\n';
            } else {
              comment += '‚ö†Ô∏è Could not generate performance report\n';
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Check performance thresholds
        run: |
          python -c "
          import json
          import sys

          # Load results
          try:
              with open('benchmarks/.benchmarks/api_results.json') as f:
                  results = json.load(f)

              # Check for regressions
              failed = False
              for benchmark in results.get('benchmarks', []):
                  if benchmark.get('stats', {}).get('mean', 0) > 1000:  # 1s threshold
                      print(f'‚ùå {benchmark[\"name\"]}: {benchmark[\"stats\"][\"mean\"]:.0f}ms')
                      failed = True

              if failed:
                  sys.exit(1)
              else:
                  print('‚úÖ All performance thresholds met')
          except Exception as e:
              print(f'‚ö†Ô∏è Could not verify thresholds: {e}')
          "

  load-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: call_coaching
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Start API server
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/call_coaching
          REDIS_URL: redis://localhost:6379
        run: |
          python -m uvicorn api.rest_server:app --host 127.0.0.1 --port 8000 &
          sleep 10

      - name: Run load test
        run: |
          locust -f tests/performance/load_test.py \
            --headless \
            --users 100 \
            --spawn-rate 5 \
            --run-time 5m \
            --host http://127.0.0.1:8000 \
            --csv=load_test_results

      - name: Run stress test
        run: |
          python tests/performance/stress_test.py

      - name: Upload load test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: load-test-results
          path: |
            load_test_results*
            stress_test_results.json
          retention-days: 30

  regression-detection:
    runs-on: ubuntu-latest
    needs: [performance-tests]
    if: github.event_name == 'pull_request'

    steps:
      - uses: actions/checkout@v4

      - name: Download baseline benchmarks
        run: |
          mkdir -p benchmarks/.benchmarks
          # In a real setup, you would download from a baseline storage
          # aws s3 cp s3://benchmarks/baseline.json benchmarks/.benchmarks/baseline.json

      - name: Detect regressions
        run: |
          python -c "
          import json
          import sys

          # This would compare current results with baseline
          # and fail if regression > 10%
          print('Checking for performance regressions...')
          # Implementation would compare benchmark results here
          "

      - name: Post regression report
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '‚ö†Ô∏è Performance regression detected. Please review the benchmark results.'
            });
